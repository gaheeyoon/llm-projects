##### ê¸°ë³¸ ì •ë³´ ì…ë ¥ ####
# ìŒì„± ë…¹ìŒì„ ìœ„í•œ ì˜¤ë””ì˜¤ ë ˆì½”ë” íŒ¨í‚¤ì§€ ì¶”ê°€
from audiorecorder import audiorecorder

# ìŠ¤íŠ¸ë¦¼ë¦¿ íŒ¨í‚¤ì§€ ì¶”ê°€
import streamlit as st

# OpenAI íŒ¨í‚¤ì§€ ì¶”ê°€
from openai import OpenAI

# íŒŒì´ì¬ ê¸°ë³¸ íŒ¨í‚¤ì§€
import os
import base64
import numpy as np

##### ê¸°ëŠ¥ êµ¬í˜„ í•¨ìˆ˜ #####
# ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” STT(Speech-to-Text) API
def STT(audio, client):
    # Whisper APIê°€ íŒŒì¼ í˜•íƒœë¡œ ìŒì„±ì„ ì…ë ¥ë°›ìœ¼ë¯€ë¡œ input.mp3 íŒŒì¼ì„ ì €ì¥
    filename='input.mp3'
    wav_file = open(filename, "wb")
    wav_file.write(audio.export().read())
    wav_file.close()

    # ìŒì„± íŒŒì¼ ì—´ê¸°
    audio_file = open(filename, "rb")
    # Whisper ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì–»ê¸°
    try:
        # openaiì˜ Whisper API ë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="text"
        )
    
        # Whisperë¡œ TTSê°€ ëë‚¬ìœ¼ë‹ˆ ì´ì œ mp3 íŒŒì¼ì„ ë‹¤ì‹œ ì‚­ì œí•©ë‹ˆë‹¤.
        audio_file.close()
        os.remove(filename)
    except:
        transcript='API Keyë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”'
    return transcript

# í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” TTS(Text-to-Speech) API
def TTS(response, client):
    # TTSë¥¼ í™œìš©í•˜ì—¬ ë§Œë“  ìŒì„±ì„ íŒŒì¼ë¡œ ì €ì¥
    with client.audio.speech.with_streaming_response.create(
        model="tts-1",
        voice="onyx",
        input=response,
    ) as response:
        filename ="output.mp3"
        response.stream_to_file(filename)

    # ì €ì¥í•œ ìŒì„± íŒŒì¼ì„ ìë™ ì¬ìƒ
    with open(filename, "rb") as f:
        data = f.read()
        b64 = base64.b64encode(data).decode()

        # ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ ìŒì„± ìë™ ì¬ìƒ
        md = f'''
            <audio autoplay="True">
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
            </audio>
        '''
        st.markdown(md, unsafe_allow_html=True,)
    # í´ë”ì— ë‚¨ì§€ ì•Šë„ë¡ íŒŒì¼ì„ ì‚­ì œ
    os.remove(filename)

# ìŒì„± ë¹„ì„œì˜ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ChatGPT API
def ask_gpt(prompt, client):
    response = client.chat.completions.create(
        model='gpt-3.5-turbo',
        messages=prompt
    )
    return response.choices[0].message.content

##### ë©”ì¸ í•¨ìˆ˜#####
def main():
    # OpenAI API í‚¤ ì§€ì •í•˜ê¸°
    client = OpenAI(api_key="ì—¬ê¸°ì— API í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”")
    
    # í™”ë©´ ìƒë‹¨ì— í‘œì‹œë  í”„ë¡œê·¸ë¨ì˜ ì´ë¦„
    st.set_page_config(
        page_title="ìŒì„± ë¹„ì„œ í”„ë¡œê·¸ë¨ğŸ”Š",
        layout="wide")
    
    # st.session_state["check_audio"]: í”„ë¡œê·¸ë¨ì´ ì¬ì‹¤í–‰ë  ë•Œë§ˆë‹¤ ì´ì „ ë…¹ìŒ íŒŒì¼ ì •ë³´ê°€ ë²„í¼ì— ë‚¨ì•„ìˆì–´ ì‹¤í–‰ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì´ì „ ë…¹ìŒ íŒŒì¼ ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    if "check_audio" not in st.session_state:
        st.session_state["check_audio"] = []
    
    # st.session_state["messages"]: GPT APIì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°ˆ í”„ë¡¬í”„íŠ¸ ì–‘ì‹. ì´ì „ ì§ˆë¬¸ ë° ë‹µë³€ì„ ëˆ„ì í•˜ì—¬ ì €ì¥.
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "system", "content": 'You are a thoughtful assistant. Respond to all input in 25 words and answer in korean'}]
    
    # ê¸°ëŠ¥ êµ¬í˜„ ê³µê°„
    col1, col2 = st.columns(2)
    
    # ì™¼ìª½ ê³µê°„ ì‘ì„±
    with col1:
        # ì œëª©
        st.header('AI Assistant ğŸ”Š')
        st.image('ai.png', width=200)
        # êµ¬ë¶„ì„ 
        st.markdown('---')
    
        # ìŒì„± ì…ë ¥ í™•ì¸ìš© í”Œë˜ê·¸
        flag_start = False
    
        # ìŒì„± ë…¹ìŒ ì•„ì´ì½˜ ì¶”ê°€
        audio = audiorecorder("ì§ˆë¬¸", "ë…¹ìŒì¤‘...")
        if len(audio) > 0 and not np.array_equal(audio, st.session_state["check_audio"]):
            # ìŒì„± ì¬ìƒ
            st.audio(audio.export().read())
        
            # ìŒì› íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
            question = STT(audio, client)
            
            # GPT ëª¨ë¸ì— ë„£ì„ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•´ ì§ˆë¬¸ì„ ì €ì¥. ì´ë•Œ ê¸°ì¡´ ë‚´ìš©ì„ ëˆ„ì .
            st.session_state["messages"] = st.session_state["messages"]+ [{"role": "user", "content": question}]
            # audio ë²„í¼ í™•ì¸ì„ ìœ„í•´ í˜„ ì‹œì ì˜ ì˜¤ë””ì˜¤ ì •ë³´ë¥¼ ì €ì¥
            st.session_state["check_audio"] = audio
            flag_start=True
    
    # ì˜¤ë¥¸ìª½ ê³µê°„ ì‘ì„±
    with col2:
        st.subheader('ëŒ€í™”ê¸°ë¡ âŒ¨')
        if flag_start:
        
            # ChatGPTì—ê²Œ ë‹µë³€ ì–»ê¸°
            response = ask_gpt(st.session_state["messages"], client)
        
            # GPT ëª¨ë¸ì— ë„£ì„ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•´ ë‹µë³€ ë‚´ìš©ì„ ì €ì¥
            st.session_state["messages"] = st.session_state["messages"]+ [{"role": "assistant", "content": response}]
        
            # Userì™€ Assistantì˜ ëŒ€í™”ë¥¼ í™”ë©´ì— ì¶œë ¥. ë‹¨, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ì œì™¸
            for message in st.session_state["messages"]:
                if message["role"] != 'system':
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])
        
            # TTSë¥¼ í™œìš©í•˜ì—¬ ìŒì„± íŒŒì¼ ìƒì„± ë° ì¬ìƒ
            TTS(response, client)

if __name__=="__main__":
    main()